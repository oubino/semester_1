{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "HRcizaNhN4j_"
   },
   "outputs": [],
   "source": [
    "# when want to test on test data you have to write\n",
    "# test_set.dataset.__test__() \n",
    "# which now means that all images loaded in (even ones from train & val would have no data augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "LzeJjDQcH_Sf"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "from PIL import Image, ImageOps\n",
    "from sklearn.metrics import f1_score\n",
    "from scipy import stats\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision.models import AlexNet\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "from skimage import io, transform\n",
    "import os\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0H04nWu_4Ja2",
    "outputId": "c981517e-cf75-48a9-d387-c206d7453c60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphviz in c:\\users\\olive\\anaconda3\\lib\\site-packages (0.15)\n",
      "Requirement already satisfied: torchviz in c:\\users\\olive\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: graphviz in c:\\users\\olive\\anaconda3\\lib\\site-packages (from torchviz) (0.15)\n",
      "Requirement already satisfied: torch in c:\\users\\olive\\anaconda3\\lib\\site-packages (from torchviz) (1.7.0)\n",
      "Requirement already satisfied: future in c:\\users\\olive\\anaconda3\\lib\\site-packages (from torch->torchviz) (0.18.2)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\olive\\anaconda3\\lib\\site-packages (from torch->torchviz) (3.7.4.2)\n",
      "Requirement already satisfied: dataclasses in c:\\users\\olive\\anaconda3\\lib\\site-packages (from torch->torchviz) (0.6)\n",
      "Requirement already satisfied: numpy in c:\\users\\olive\\anaconda3\\lib\\site-packages (from torch->torchviz) (1.18.5)\n"
     ]
    }
   ],
   "source": [
    "# import for visualisation of network\n",
    "!pip install graphviz\n",
    "!pip install torchviz\n",
    "import graphviz\n",
    "import torchviz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c7YTGs7DIIF7",
    "outputId": "6d8d2fe6-9986-4457-9614-d94c51f7be89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Invalid combination of input arguments. Please run 'nvidia-smi -h' for help.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use GPU\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device) # if on CUDA machine -> should print a CUDA device:\n",
    "!nvidia-smi  # tells you what GPU you're on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v0tnznaEILdz",
    "outputId": "3c1bb8f4-5a8b-456c-88fa-678ac039ee9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olive\\OneDrive\\Documents\\HNSCC_slices\n",
      "C:\\Users\\olive\\OneDrive\\Documents\\HNSCC_slices\n"
     ]
    }
   ],
   "source": [
    "# change path to correct folder \n",
    "\n",
    "\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')\n",
    "#PATH_OF_DATA= '/content/gdrive/\"My Drive\"/data/\"HNSCC_slices\"' # see content of dataset\n",
    "#!ls {PATH_OF_DATA} # datasets.ImageFolder() expects data as root/label/picture.png\n",
    "#root = '/content/gdrive/My Drive/data/HNSCC_slices' # note lack of \" \"\n",
    "\n",
    "\n",
    "# if doing locally \n",
    "\n",
    "\n",
    "# computer\n",
    "\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')\n",
    "#PATH_OF_DATA = r'C:\\Users\\olive\\OneDrive\\Documents\\HNSCC_slices-20201116T001636Z-001.zip\\HNSCC_slices' # see content of dataset\n",
    "#!ls {PATH_OF_DATA} # datasets.ImageFolder() expects data as root/label/picture.png\n",
    "root = r'C:\\Users\\olive\\OneDrive\\Documents\\HNSCC_slices' # note lack of \" \"\n",
    "import os\n",
    "print(os.getcwd())\n",
    "os.chdir(root)\n",
    "print(os.getcwd())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define normalisation \n",
    "norm_mean = 1024+50\n",
    "norm_std = 1000\n",
    "#mask_mean = 0.9346\n",
    "#mask_centre_mean = 0.1538"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "oOa-DS2AITA1"
   },
   "outputs": [],
   "source": [
    "# custom dataset class for the images and masks\n",
    "\n",
    "import cv2\n",
    "import random\n",
    "import sklearn\n",
    "import skimage.transform as tr\n",
    "import skimage\n",
    "\n",
    "def mask_to_point(mask):\n",
    "  x_coord = 0\n",
    "  y_coord = 0\n",
    "  counter = 0\n",
    "  for i in range(mask.shape[0]):\n",
    "    for j in range(mask.shape[1]):\n",
    "      if (mask[i][j] == 1):\n",
    "        x_coord += i\n",
    "        y_coord += j\n",
    "        counter += 1\n",
    "\n",
    "  x_one = int(x_coord/counter)\n",
    "  y_one = int(y_coord/counter)\n",
    "  mask = np.zeros(shape = mask.shape)\n",
    "  mask[x_one][y_one] = 1\n",
    "  return mask\n",
    "\n",
    "class SliceDataset(Dataset):\n",
    "    \"\"\"2D CT Scan dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root, transform_train =None, transform_test = None, test = False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"CTs\")))) # ensure they're aligned & index them\n",
    "        self.masks = list(sorted(os.listdir(os.path.join(root, \"Masks\"))))\n",
    "        self.masks_centres = list(sorted(os.listdir(os.path.join(root, \"Mask Centres\"))))\n",
    "        self.transform_train = transform_train\n",
    "        self.transform_test = transform_test\n",
    "        self.test = False\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx): # convert tensor to list to index items\n",
    "            idx = idx.tolist() \n",
    "        img_path = os.path.join(self.root, \"CTs\", self.imgs[idx]) # image path is combination of root and index \n",
    "        mask_path = os.path.join(self.root, \"Masks\", self.masks[idx])\n",
    "        mask_centre_path = os.path.join(self.root, \"Mask Centres\", self.masks_centres[idx])\n",
    "\n",
    "        img = np.load(img_path) # image read in as numpy array\n",
    "        mask = np.load(mask_path) # mask - think 0 = background\n",
    "        mask_centre = np.load(mask_centre_path)\n",
    "        \n",
    "        #print('img size', img.shape)\n",
    "\n",
    "        sample = {'image': img, 'mask': mask, 'mask_centre': mask_centre} # both are nd.arrays, stored in sample dataset\n",
    "\n",
    "        if (self.transform_train) and (self.test == False):\n",
    "            sample = self.transform_train(sample) # if transforms present, act on sample\n",
    "        if (self.transform_test) and (self.test == True):\n",
    "            sample = self.transform_test(sample) # if transforms present, act on sample\n",
    "        if (mask.max() != mask.min()): # exclude images where no mask\n",
    "          #print('mask_max:%5.2f, mask_min:%5.2f' % (mask.max(),mask.min()))\n",
    "          #print('x where its equal to 1')\n",
    "          #print((np.where(mask == mask.max())[0]))\n",
    "          return sample \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs) # get size of dataset\n",
    "\n",
    "    def __test__(self):\n",
    "      self.test = True\n",
    "    \n",
    "    def __train__(self):\n",
    "      self.train = False\n",
    "\n",
    "\n",
    "class DatasetFromSubset(Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.subset[index]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "\n",
    "class Rescale(object): # need to change rescale so longer side is matched to int and then pad\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple)) # int keeps aspect ratio the same\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, mask, mask_centre = sample['image'], sample['mask'], sample['mask_centre'] # getting mask and image from sample so can operate on them\n",
    "        #print('before resize')\n",
    "        #print(mask.max())\n",
    "        #print(mask.min())\n",
    "        h, w = image.shape[:2] # define image height and width as first two values in shape\n",
    "        if isinstance(self.output_size, int): # maintain aspect ratio so no loss of info\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        img = transform.resize(image, (new_h, new_w),preserve_range = True, anti_aliasing = True) # anti-aliasing was false\n",
    "        # h and w are swapped for mask because for images,\n",
    "        # x and y axes are axis 1 and 0 respectively\n",
    "        mask = transform.resize(mask, (new_h,new_w), preserve_range = True,  anti_aliasing = True) # anti-aliasing was false\n",
    "        mask_centre =  transform.resize(mask_centre, (new_h,new_w), preserve_range = True, anti_aliasing = True)\n",
    "        #print('after resize')\n",
    "        #print(mask.max())\n",
    "        #print(mask.min())\n",
    "        return {'image': img, 'mask': mask, 'mask_centre': mask_centre}\n",
    "\n",
    "class Normalise(object):  \n",
    "  \"\"\" Normalise CT scan in the desired examination window\n",
    "      takes in image as numpy \"\"\"\n",
    "  \n",
    "  def __init__(self, level, window):\n",
    "      self.level = level\n",
    "      self.window = window\n",
    "                          \n",
    "  def __call__(self, sample):\n",
    "      image, mask, mask_centre = sample['image'], sample['mask'], sample['mask_centre']\n",
    "      # image\n",
    "      minval = self.level - self.window/2\n",
    "      maxval = self.level + self.window/2\n",
    "      img_norm = np.clip(image, minval, maxval)\n",
    "      img_norm -= minval\n",
    "      img_norm /= self.window\n",
    "      return {'image':img_norm, 'mask': mask, 'mask_centre': mask_centre} # note note !\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, mask, mask_centre = sample['image'], sample['mask'], sample['mask_centre']\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        image = torch.from_numpy(image).float() # dont know why images/mask casted to float here but need to do it again later\n",
    "        mask = torch.from_numpy(mask).float()\n",
    "        mask = mask.unsqueeze(0) # force mask to have extra dimension i.e. (1xHxW)\n",
    "        image = image.unsqueeze(0)\n",
    "        mask_centre = torch.from_numpy(mask_centre).float()\n",
    "        mask_centre = mask_centre.unsqueeze(0)\n",
    "        return {'image': image,'mask': mask, 'mask_centre': mask_centre}\n",
    "\n",
    "class HorizontalFlip(object):\n",
    "  \"\"\" Flip images \"\"\"\n",
    "  def __call__(self,sample):\n",
    "    image, mask,mask_centre = sample['image'], sample['mask'], sample['mask_centre']\n",
    "    if random.random() <= 0.5:\n",
    "      transform = transforms.Compose([transforms.RandomHorizontalFlip(p=1)])\n",
    "      image = transform(image)\n",
    "      mask = transform(mask)\n",
    "      mask_centre = transform(mask_centre)\n",
    "    return {'image': image, 'mask': mask, 'mask_centre': mask_centre}\n",
    "\n",
    "class Rotation(object):  \n",
    "  \"\"\" Random rotate images \"\"\"\n",
    "  def __call__(self,sample):\n",
    "    image, mask, mask_centre = sample['image'], sample['mask'], sample['mask_centre']\n",
    "    if random.random() <= 0.5:\n",
    "        angle = random.randint(-10, 10)\n",
    "        image = tr.rotate(image, angle)\n",
    "        mask = tr.rotate(mask, angle)\n",
    "        mask_centre = tr.rotate(mask_centre, angle)\n",
    "    return {'image': image, 'mask': mask, 'mask_centre': mask_centre}\n",
    "\n",
    "\n",
    "class Shifting(object): \n",
    "  \"\"\" Random shift images \"\"\"\n",
    "  def __call__(self,sample):\n",
    "    image, mask,mask_centre = sample['image'], sample['mask'], sample['mask_centre']\n",
    "    if random.random() <= 0.5:\n",
    "        translation_x = random.randint(-30, 30)\n",
    "        translation_y = random.randint(-30,30)\n",
    "        transform = tr.AffineTransform(translation = (translation_x,translation_y))\n",
    "        image = tr.warp(image,transform) # default is padding with zeros\n",
    "        mask = tr.warp(mask,transform)\n",
    "        mask_centre = tr.warp(mask_centre,transform)\n",
    "    return {'image': image, 'mask': mask, 'mask_centre': mask_centre}\n",
    "\n",
    "class Noise(object):  # helps prevent overfitting\n",
    "  \"\"\" Random noise images \"\"\"\n",
    "  def __call__(self,sample):\n",
    "    image, mask,mask_centre = sample['image'], sample['mask'], sample['mask_centre']\n",
    "    if random.random() <= 0.5:\n",
    "      image = skimage.util.random_noise(image, mean = 0, var = 0.0000000001, clip = False)\n",
    "      # would this work given that mask is unchanged??\n",
    "    return {'image': image, 'mask': mask, 'mask_centre': mask_centre}\n",
    "\n",
    "class GaussBlur(object):  # helps prevent overfitting\n",
    "  \"\"\" Gaussian blur images \"\"\"\n",
    "  def __call__(self,sample):\n",
    "    image, mask, mask_centre = sample['image'], sample['mask'],sample['mask_centre']\n",
    "    if random.random() <= 0.5:\n",
    "      image = cv2.GaussianBlur(image,(3,3),0)\n",
    "      # do i want to blur the mask??\n",
    "    return {'image': image, 'mask': mask, 'mask_centre':mask_centre}\n",
    "\n",
    "\n",
    "def coords_max(heatmap): # assumes 1 channel\n",
    "  # heatmap is batch of either masks or image of \n",
    "  # heatmap dim is (B x C x H x W)\n",
    "  # output is (B x coord)\n",
    "  # i.e. x,y of 3rd image = coords[3][0],coords[3][1] only 1 channel\n",
    "  b = torch.max(heatmap,dim =3,keepdim = True)\n",
    "  c = torch.max(b[0],dim = 2,keepdim = True)\n",
    "  batch_size = (heatmap.size()[0])\n",
    "  for i in range(batch_size):\n",
    "    y = c[1][i][0][0][0]\n",
    "    x = b[1][i][0][y][0]\n",
    "    if i == 0:\n",
    "      coords = torch.tensor([[x,y]])\n",
    "    else: \n",
    "      coords_temp = torch.tensor([[x,y]])\n",
    "      coords = torch.cat((coords,coords_temp),dim = 0)\n",
    "\n",
    "  return coords      \n",
    "\n",
    "from scipy import optimize\n",
    "\n",
    "def gaussian_new(height, center_x, center_y, width_x, width_y):\n",
    "    \"\"\"Returns a gaussian function with the given parameters\"\"\"\n",
    "    width_x = float(width_x)\n",
    "    width_y = float(width_y)\n",
    "    return lambda x,y: height*np.exp(\n",
    "                -(((center_x-x)/width_x)**2+((center_y-y)/width_y)**2)/2)\n",
    "\n",
    "def moments(data):\n",
    "    \"\"\"Returns (height, x, y, width_x, width_y)\n",
    "    the gaussian parameters of a 2D distribution by calculating its\n",
    "    moments \"\"\"\n",
    "    total = data.sum()\n",
    "    X, Y = np.indices(data.shape)\n",
    "    x = (X*data).sum()/total\n",
    "    y = (Y*data).sum()/total\n",
    "    col = data[:, int(y)]\n",
    "    width_x = np.sqrt(np.abs((np.arange(col.size)-x)**2*col).sum()/col.sum())\n",
    "    row = data[int(x), :]\n",
    "    width_y = np.sqrt(np.abs((np.arange(row.size)-y)**2*row).sum()/row.sum())\n",
    "    height = data.max()\n",
    "    return height, x, y, width_x, width_y\n",
    "\n",
    "def fitgaussian(data):\n",
    "    \"\"\"Returns (height, x, y, width_x, width_y)\n",
    "    the gaussian parameters of a 2D distribution found by a fit\"\"\"\n",
    "    params = moments(data)\n",
    "    errorfunction = lambda p: np.ravel(gaussian_new(*p)(*np.indices(data.shape)) -\n",
    "                                 data)\n",
    "    p, success = optimize.leastsq(errorfunction, params)\n",
    "    return p\n",
    "\n",
    "# fit 2D gauss and find maximum of it - just use for performance metrics as for other will be slow\n",
    "def gauss_max(heatmap): # assumes 1 channel\n",
    "    # heatmap is batch of masks or images of dim (B x C x H x W)\n",
    "    # output is (B x coord)\n",
    "    # i.e. x,y of 3rd image = coords[3][0], coords[3][1] only 1 channel\n",
    "    batch_size = (heatmap.size()[0])\n",
    "    heatmap = heatmap.detach().cpu().numpy()\n",
    "    for i in range(batch_size):\n",
    "        params = fitgaussian(heatmap[i][0])\n",
    "        x = params[2]\n",
    "        y =  params[1]  \n",
    "        x_max = heatmap.shape[3]\n",
    "        y_max = heatmap.shape[2]\n",
    "        x = max(0,min(x,x_max)) # keep x & y in [0,224]\n",
    "        y = max(0,min(y,y_max))\n",
    "        if i == 0:\n",
    "            coords = torch.tensor([[x,y]])\n",
    "        else: \n",
    "            coords_temp = torch.tensor([[x,y]])\n",
    "            coords = torch.cat((coords,coords_temp),dim = 0)\n",
    "    return coords    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "RvSQDvZTL2VI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmask_list = list(sorted(os.listdir(os.path.join(root, \"Masks\"))))\\n  \\nfor idx in range(1778):  \\n  mask_path = os.path.join(root, \"Masks\", mask_list[idx])\\n  mask = np.load(mask_path) # mask - think 0 = background\\n  mask_centre = mask_to_point(mask)\\n  model_save_name = (mask_list[idx])\\n  path = F\"/content/gdrive/My Drive/data/HNSCC_slices/Mask Centres/{model_save_name}\" \\n  np.save(path, mask_centre) \\n\\n'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "mask_list = list(sorted(os.listdir(os.path.join(root, \"Masks\"))))\n",
    "  \n",
    "for idx in range(1778):  \n",
    "  mask_path = os.path.join(root, \"Masks\", mask_list[idx])\n",
    "  mask = np.load(mask_path) # mask - think 0 = background\n",
    "  mask_centre = mask_to_point(mask)\n",
    "  model_save_name = (mask_list[idx])\n",
    "  path = F\"/content/gdrive/My Drive/data/HNSCC_slices/Mask Centres/{model_save_name}\" \n",
    "  np.save(path, mask_centre) \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O3LDXUD9H8l4",
    "outputId": "8ca31808-3894-49dc-e148-848b7b85d1df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img size (480, 480)\n",
      "torch.Size([1, 224, 224])\n",
      "img size (480, 480)\n",
      "torch.Size([1, 224, 224])\n",
      "img size (480, 480)\n",
      "torch.Size([1, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# transformations and normalization\n",
    "\n",
    "#tensor(1067.6926)\n",
    "#tensor(0.9346)\n",
    "#tensor(0.1538)\"\n",
    "\n",
    "\n",
    "# rotation gaussblu noise\n",
    "# same transformations for train/val/test\n",
    "trans_plain = transforms.Compose([Rescale(224),Normalise(norm_mean,norm_std),ToTensor()]) # trans 1 # was 0.0326, 1\n",
    "#trans = transforms.Compose([Rescale(224),Normalise(0.0326,1),ToTensor(), HorizontalFlip()]) # trans 2\n",
    "trans_augment = transforms.Compose([Rescale(224),Rotation(),  GaussBlur(),Noise(), Shifting(), Normalise(norm_mean,norm_std),ToTensor(), HorizontalFlip()])# trans 3\n",
    "# note how unsure about noise & gaussian blur -> how to implement\n",
    "# how to implement erasing etc?\n",
    "# note warning here for perspective distortion\n",
    "\n",
    "dataset = SliceDataset(root, transform_train = trans_augment, transform_test = trans_plain, test = False )\n",
    "\n",
    "# split data in train/val/test\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_set, val_set, test_set = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "\n",
    "image_datasets = {\n",
    "    'train': train_set, 'val': val_set, 'test':test_set\n",
    "}\n",
    "batch_size = 1\n",
    "\n",
    "# Load data in\n",
    "dataloaders = {\n",
    "    'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n",
    "    'val': DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=0),\n",
    "    'test': DataLoader(test_set,batch_size = batch_size, shuffle = False, num_workers=0)\n",
    "}\n",
    "\n",
    "\n",
    "print(train_set.__getitem__(0)['image'].size()) # i.e. 1 x 224 x 224 as torch tensor (C x H x W)\n",
    "print(train_set.__getitem__(0)['mask'].size()) \n",
    "print(train_set.__getitem__(0)['mask_centre'].size()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Sq_MOaSDJr9J",
    "outputId": "0292b108-381b-4446-c09c-c9c144f4581d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#from matplotlib.pyplot import figure\\n#figure(figsize=(8, 6), dpi=80)\\n\\nfor batch in dataloaders['train']:\\n    image = batch['image']\\n    mask = batch['mask']\\n    mask_centre = batch['mask_centre']\\n    for k in range(image.size(0)):\\n      fig = plt.figure(figsize=(7,7), dpi = 80)\\n      ax = fig.add_subplot(111)\\n      x, y = coords_max(mask_centre)[k][0],coords_max(mask_centre)[k][1]\\n      plot_image = reverse_transform(image[k])\\n      plot_mask = reverse_transform(mask[k])\\n      plot_mask_centre = reverse_transform(mask_centre[k])\\n      plt.imshow(plot_image, cmap='Greys_r',vmin=plot_image.min(), vmax=plot_image.max(),alpha = 1.0)\\n      plt.imshow(plot_mask,cmap='Reds', alpha = 0.2)\\n      #plt.imshow(plot_mask_centre, cmap = 'Blues', alpha = 0.5)\\n      plt.show()\\n\\n\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check data loaded in correctly\n",
    "\n",
    "import torchvision.utils\n",
    "\n",
    "def reverse_transform(img): # to plot original image loaded in \n",
    "    # could do reverse of normalisation here\n",
    "    img = img.numpy().transpose((1,2,0))\n",
    "    img_numpy = np.squeeze(img) # gets rid of dead channel axis (H x W x 1)\n",
    "    return img_numpy\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "#from matplotlib.pyplot import figure\n",
    "#figure(figsize=(8, 6), dpi=80)\n",
    "\n",
    "for batch in dataloaders['train']:\n",
    "    image = batch['image']\n",
    "    mask = batch['mask']\n",
    "    mask_centre = batch['mask_centre']\n",
    "    for k in range(image.size(0)):\n",
    "      fig = plt.figure(figsize=(7,7), dpi = 80)\n",
    "      ax = fig.add_subplot(111)\n",
    "      x, y = coords_max(mask_centre)[k][0],coords_max(mask_centre)[k][1]\n",
    "      plot_image = reverse_transform(image[k])\n",
    "      plot_mask = reverse_transform(mask[k])\n",
    "      plot_mask_centre = reverse_transform(mask_centre[k])\n",
    "      plt.imshow(plot_image, cmap='Greys_r',vmin=plot_image.min(), vmax=plot_image.max(),alpha = 1.0)\n",
    "      plt.imshow(plot_mask,cmap='Reds', alpha = 0.2)\n",
    "      #plt.imshow(plot_mask_centre, cmap = 'Blues', alpha = 0.5)\n",
    "      plt.show()\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g1c2VAA4tJwy",
    "outputId": "29c279d0-532b-41b0-c50f-caedd836a53c"
   },
   "outputs": [],
   "source": [
    "\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class UNET(nn.Module): # need to add bottleneck\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = self.contract_block(in_channels, 64, 3, 1)\n",
    "        self.conv2 = self.contract_block(64, 128, 3, 1)\n",
    "        self.conv3 = self.contract_block(128, 128, 3, 1)\n",
    "        self.conv4 = self.contract_block(128, 128, 3, 1)\n",
    "        self.dropout = nn.Dropout2d(p=0.5)\n",
    "        self.pool = self.pooling_layer()\n",
    "\n",
    "        self.bottleneck = self.bottleneck_block(128, 256, 128, 3, 1)\n",
    "\n",
    "        self.upconv4 = self.expand_block(128*2, 128, 3, 1)\n",
    "        self.upscale_4 = self.upscale_layer(128)\n",
    "        self.upconv3 = self.expand_block(128*2, 128, 3, 1)\n",
    "        self.upscale_3 = self.upscale_layer(128)\n",
    "        self.upconv2 = self.expand_block(128*2, 64, 3, 1)\n",
    "        self.upscale_2 = self.upscale_layer(64)\n",
    "        self.final_conv = self.final_block(64*2, out_channels, 3, 1)\n",
    "\n",
    "\n",
    "\n",
    "    def __call__(self, x):\n",
    "\n",
    "        # downsampling part\n",
    "        # encode the input image into feature representations at multiple different levels.\n",
    "        conv1 = self.conv1(x)\n",
    "        pool1 = self.pool(conv1)\n",
    "        conv2 = self.conv2(pool1)\n",
    "        #drop1 = self.dropout(conv2)\n",
    "        pool2 = self.pool(conv2)\n",
    "        conv3 = self.conv3(pool2)\n",
    "        #drop2 = self.dropout(conv3)\n",
    "        pool3 = self.pool(conv3)\n",
    "        conv4 = self.conv4(pool3)\n",
    "        #drop2 = self.dropout(conv3)\n",
    "        \n",
    "\n",
    "        # bottleneck\n",
    "        # force the model to learn a compression of the input data.\n",
    "        # compressed view should only contain the “useful” information to\n",
    "        # reconstruct the input (or segmentation map).\n",
    "        pool4 = self.pool(conv4)\n",
    "        bottleneck1 = self.bottleneck(pool4)\n",
    "        #bottleneck2 = self.bottleneck(bottleneck1\n",
    "\n",
    "        # expand block concatenate conv4 and bottleneck output\n",
    "\n",
    "        upconv4 = self.upconv4(torch.cat([bottleneck1, conv4], 1))\n",
    "        upscale_conv_4 = self.upscale_4(upconv4)\n",
    "        upconv3 = self.upconv3(torch.cat([upscale_conv_4, conv3], 1))\n",
    "        upscale_conv_3 = self.upscale_3(upconv3)\n",
    "        upconv2 = self.upconv2(torch.cat([upscale_conv_3, conv2], 1))\n",
    "        upscale_conv_2 = self.upscale_2(upconv2)\n",
    "        final_layer = self.final_conv(torch.cat([upscale_conv_2, conv1], 1))\n",
    "\n",
    "        return final_layer\n",
    "\n",
    "\n",
    "    def contract_block(self, in_channels, out_channels, kernel_size, padding):\n",
    "\n",
    "        contract = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=1, padding=padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, stride=1, padding=padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "                                 )\n",
    "\n",
    "        return contract\n",
    "\n",
    "    def expand_block(self, in_channels, out_channels, kernel_size, padding):\n",
    "\n",
    "        expand = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size, stride=1, padding=padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "                            )\n",
    "        return expand\n",
    "\n",
    "    def bottleneck_block(self, in_channels, mid_channels, out_channels, kernel_size, padding):\n",
    "\n",
    "      bottleneck = nn.Sequential(\n",
    "          nn.Conv2d(in_channels, mid_channels, kernel_size = kernel_size, stride = 1, padding = padding),#, stride = 1, padding =1),\n",
    "          nn.ReLU(),\n",
    "          nn.BatchNorm2d(mid_channels),\n",
    "          nn.Conv2d(mid_channels, mid_channels, kernel_size=kernel_size, stride=1, padding=padding),\n",
    "          nn.ReLU(),\n",
    "          nn.BatchNorm2d(mid_channels),\n",
    "          nn.ConvTranspose2d(mid_channels, out_channels, kernel_size = 3, stride = 2, padding = 1, output_padding=1) #??\n",
    "          )\n",
    "      return bottleneck\n",
    "    \n",
    "    def pooling_layer(self):\n",
    "\n",
    "      pool = nn.Sequential (\n",
    "          nn.AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
    "      )\n",
    "      return pool\n",
    "\n",
    "    def upscale_layer(self, channels):\n",
    "\n",
    "      upscale = nn.Sequential(\n",
    "          nn.ConvTranspose2d(channels, channels, kernel_size = 3, stride = 2, padding = 1, output_padding=1) #??\n",
    "      )\n",
    "      return upscale\n",
    "\n",
    "\n",
    "    def final_block(self, in_channels, out_channels, kernel_size, padding):\n",
    "\n",
    "        final_block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=1, padding=padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, stride=1, padding=padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        return final_block\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "cellView": "form",
    "id": "RqlMUqDivOvQ"
   },
   "outputs": [],
   "source": [
    "#@title Ed Network\n",
    "  \n",
    "# from Ed try this out\n",
    "class Enc_Dec_2D(nn.Module):\n",
    "    def __init__(self, filter_factor=1, targets=1, in_channels=1):\n",
    "        super(Enc_Dec_2D, self).__init__()\n",
    "        ff = filter_factor # filter factor (easy net scaling)\n",
    "        # Input --> (in_channels, x, x) --> x needs to be a factor of 4 (downsampled by x0.5 twice)\n",
    "        # conv layers set 1 - down 1\n",
    "        self.c1 = nn.Conv2d(in_channels=in_channels, out_channels=int(16*ff), kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(int(16*ff))\n",
    "        self.drop1 = nn.Dropout2d(p=0.5)\n",
    "        self.c2 = nn.Conv2d(in_channels=int(16*ff), out_channels=int(32*ff), kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(int(32*ff))\n",
    "        self.drop2 = nn.Dropout2d(p=0.5)\n",
    "        # conv layers set 2 - down 2\n",
    "        self.c3 = nn.Conv2d(in_channels=int(32*ff), out_channels=int(32*ff), kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(int(32*ff))\n",
    "        self.drop3 = nn.Dropout2d(p=0.5)\n",
    "        self.c4 = nn.Conv2d(in_channels=int(32*ff), out_channels=int(64*ff), kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(int(64*ff))\n",
    "        self.drop4 = nn.Dropout2d(p=0.5)\n",
    "        # conv layers set 3 - base\n",
    "        self.c5 = nn.Conv2d(in_channels=int(64*ff), out_channels=int(64*ff), kernel_size=3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(int(64*ff))\n",
    "        self.drop5 = nn.Dropout2d(p=0.5)\n",
    "        self.c6 = nn.Conv2d(in_channels=int(64*ff), out_channels=int(64*ff), kernel_size=3, padding=1)\n",
    "        self.bn6 = nn.BatchNorm2d(int(64*ff))\n",
    "        self.drop6 = nn.Dropout2d(p=0.5)\n",
    "        # upsample 1\n",
    "        self.rc_1 = nn.Conv2d(in_channels=int(64*ff), out_channels=int(64*ff), kernel_size=3, padding=1)\n",
    "        self.bn_r1 = nn.BatchNorm2d(int(64*ff))\n",
    "        self.drop_r1 = nn.Dropout2d(p=0.5)\n",
    "        # conv layer set 4 - up 1\n",
    "        self.c7 = nn.Conv2d(in_channels=int(64*ff), out_channels=int(32*ff), kernel_size=3, padding=1)\n",
    "        self.bn7 = nn.BatchNorm2d(int(32*ff))\n",
    "        self.drop7 = nn.Dropout2d(p=0.5)\n",
    "        self.c8 = nn.Conv2d(in_channels=int(32*ff), out_channels=int(32*ff), kernel_size=3, padding=1)\n",
    "        self.bn8 = nn.BatchNorm2d(int(32*ff))\n",
    "        self.drop8 = nn.Dropout2d(p=0.5)\n",
    "        # upsample 2\n",
    "        self.rc_2 = nn.Conv2d(in_channels=int(32*ff), out_channels=int(32*ff), kernel_size=3, padding=1)\n",
    "        self.bn_r2 = nn.BatchNorm2d(int(32*ff))\n",
    "        self.drop_r2 = nn.Dropout2d(p=0.5)\n",
    "        # conv layer set 5 - up 2\n",
    "        self.c9 = nn.Conv2d(in_channels=int(32*ff), out_channels=int(16*ff), kernel_size=3, padding=1)\n",
    "        self.bn9 = nn.BatchNorm2d(int(16*ff))\n",
    "        self.drop9 = nn.Dropout2d(p=0.5)\n",
    "        self.c10 = nn.Conv2d(in_channels=int(16*ff), out_channels=int(16*ff), kernel_size=3, padding=1)\n",
    "        self.bn10 = nn.BatchNorm2d(int(16*ff))\n",
    "        self.drop10 = nn.Dropout2d(p=0.5)\n",
    "        # prediction convolution\n",
    "        self.pred = nn.Conv2d(in_channels=int(16*ff), out_channels=int(targets), kernel_size=1)\n",
    "    \n",
    "    def forward(self, im):\n",
    "        # Down block 1\n",
    "        x = F.relu(self.bn1(self.c1(im)))\n",
    "        x = self.drop1(x)\n",
    "        x = F.relu(self.bn2(self.c2(x)))\n",
    "        skip1 = self.drop2(x)\n",
    "        x = F.max_pool2d(skip1, (2,2))\n",
    "        # Down block 2\n",
    "        x = F.relu(self.bn3(self.c3(x)))\n",
    "        x = self.drop3(x)\n",
    "        x = F.relu(self.bn4(self.c4(x)))\n",
    "        skip2 = self.drop4(x)\n",
    "        x = F.max_pool2d(skip2, (2,2))\n",
    "        # Base block \n",
    "        x = F.relu(self.bn5(self.c5(x)))\n",
    "        x = self.drop5(x)\n",
    "        x = F.relu(self.bn6(self.c6(x)))\n",
    "        x = self.drop6(x)\n",
    "        # Upsample 1\n",
    "        x = F.relu(self.bn_r1(self.rc_1(F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False))))\n",
    "        x = self.drop_r1(x)\n",
    "        # Up block 1\n",
    "        x = F.relu(self.bn7(self.c7(x+skip2)))\n",
    "        x = self.drop7(x)\n",
    "        x = F.relu(self.bn8(self.c8(x)))\n",
    "        x = self.drop8(x)\n",
    "        # Upsample 2\n",
    "        x = F.relu(self.bn_r2(self.rc_2(F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False))))\n",
    "        x = self.drop_r2(x)\n",
    "        # Up block 2\n",
    "        x = F.relu(self.bn9(self.c9(x+skip1)))\n",
    "        x = self.drop9(x)\n",
    "        x = F.relu(self.bn10(self.c10(x)))\n",
    "        x = self.drop10(x)\n",
    "        # Predict\n",
    "        return self.pred(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Sequential: 1-1                        --\n",
      "|    └─Conv2d: 2-1                       640\n",
      "|    └─BatchNorm2d: 2-2                  128\n",
      "|    └─ReLU: 2-3                         --\n",
      "├─Sequential: 1-2                        --\n",
      "|    └─Conv2d: 2-4                       73,856\n",
      "|    └─BatchNorm2d: 2-5                  256\n",
      "|    └─ReLU: 2-6                         --\n",
      "├─Sequential: 1-3                        --\n",
      "|    └─Conv2d: 2-7                       147,584\n",
      "|    └─BatchNorm2d: 2-8                  256\n",
      "|    └─ReLU: 2-9                         --\n",
      "├─Sequential: 1-4                        --\n",
      "|    └─AvgPool2d: 2-10                   --\n",
      "├─Sequential: 1-5                        --\n",
      "|    └─Conv2d: 2-11                      147,584\n",
      "|    └─BatchNorm2d: 2-12                 256\n",
      "|    └─ReLU: 2-13                        --\n",
      "├─Sequential: 1-6                        --\n",
      "|    └─Conv2d: 2-14                      147,584\n",
      "|    └─BatchNorm2d: 2-15                 256\n",
      "|    └─ReLU: 2-16                        --\n",
      "├─Sequential: 1-7                        --\n",
      "|    └─Conv2d: 2-17                      147,584\n",
      "|    └─BatchNorm2d: 2-18                 256\n",
      "|    └─ReLU: 2-19                        --\n",
      "├─Sequential: 1-8                        --\n",
      "|    └─AvgPool2d: 2-20                   --\n",
      "├─Sequential: 1-9                        --\n",
      "|    └─Conv2d: 2-21                      147,584\n",
      "|    └─BatchNorm2d: 2-22                 256\n",
      "|    └─ReLU: 2-23                        --\n",
      "├─Sequential: 1-10                       --\n",
      "|    └─Conv2d: 2-24                      147,584\n",
      "|    └─BatchNorm2d: 2-25                 256\n",
      "|    └─ReLU: 2-26                        --\n",
      "├─Sequential: 1-11                       --\n",
      "|    └─Conv2d: 2-27                      147,584\n",
      "|    └─BatchNorm2d: 2-28                 256\n",
      "|    └─ReLU: 2-29                        --\n",
      "├─Sequential: 1-12                       --\n",
      "|    └─ConvTranspose2d: 2-30             147,584\n",
      "├─Sequential: 1-13                       --\n",
      "|    └─ConvTranspose2d: 2-31             147,584\n",
      "├─Sequential: 1-14                       --\n",
      "|    └─Conv2d: 2-32                      1,153\n",
      "|    └─BatchNorm2d: 2-33                 2\n",
      "|    └─ReLU: 2-34                        --\n",
      "├─Sequential: 1-15                       --\n",
      "|    └─AvgPool2d: 2-35                   --\n",
      "├─Sequential: 1-16                       --\n",
      "|    └─Conv2d: 2-36                      640\n",
      "|    └─BatchNorm2d: 2-37                 128\n",
      "|    └─ReLU: 2-38                        --\n",
      "├─Sequential: 1-17                       --\n",
      "|    └─Conv2d: 2-39                      73,856\n",
      "|    └─BatchNorm2d: 2-40                 256\n",
      "|    └─ReLU: 2-41                        --\n",
      "├─Sequential: 1-18                       --\n",
      "|    └─Conv2d: 2-42                      147,584\n",
      "|    └─BatchNorm2d: 2-43                 256\n",
      "|    └─ReLU: 2-44                        --\n",
      "├─Sequential: 1-19                       --\n",
      "|    └─ConvTranspose2d: 2-45             1,153\n",
      "=================================================================\n",
      "Total params: 1,629,956\n",
      "Trainable params: 1,629,956\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "├─Sequential: 1-1                        --\n",
       "|    └─Conv2d: 2-1                       640\n",
       "|    └─BatchNorm2d: 2-2                  128\n",
       "|    └─ReLU: 2-3                         --\n",
       "├─Sequential: 1-2                        --\n",
       "|    └─Conv2d: 2-4                       73,856\n",
       "|    └─BatchNorm2d: 2-5                  256\n",
       "|    └─ReLU: 2-6                         --\n",
       "├─Sequential: 1-3                        --\n",
       "|    └─Conv2d: 2-7                       147,584\n",
       "|    └─BatchNorm2d: 2-8                  256\n",
       "|    └─ReLU: 2-9                         --\n",
       "├─Sequential: 1-4                        --\n",
       "|    └─AvgPool2d: 2-10                   --\n",
       "├─Sequential: 1-5                        --\n",
       "|    └─Conv2d: 2-11                      147,584\n",
       "|    └─BatchNorm2d: 2-12                 256\n",
       "|    └─ReLU: 2-13                        --\n",
       "├─Sequential: 1-6                        --\n",
       "|    └─Conv2d: 2-14                      147,584\n",
       "|    └─BatchNorm2d: 2-15                 256\n",
       "|    └─ReLU: 2-16                        --\n",
       "├─Sequential: 1-7                        --\n",
       "|    └─Conv2d: 2-17                      147,584\n",
       "|    └─BatchNorm2d: 2-18                 256\n",
       "|    └─ReLU: 2-19                        --\n",
       "├─Sequential: 1-8                        --\n",
       "|    └─AvgPool2d: 2-20                   --\n",
       "├─Sequential: 1-9                        --\n",
       "|    └─Conv2d: 2-21                      147,584\n",
       "|    └─BatchNorm2d: 2-22                 256\n",
       "|    └─ReLU: 2-23                        --\n",
       "├─Sequential: 1-10                       --\n",
       "|    └─Conv2d: 2-24                      147,584\n",
       "|    └─BatchNorm2d: 2-25                 256\n",
       "|    └─ReLU: 2-26                        --\n",
       "├─Sequential: 1-11                       --\n",
       "|    └─Conv2d: 2-27                      147,584\n",
       "|    └─BatchNorm2d: 2-28                 256\n",
       "|    └─ReLU: 2-29                        --\n",
       "├─Sequential: 1-12                       --\n",
       "|    └─ConvTranspose2d: 2-30             147,584\n",
       "├─Sequential: 1-13                       --\n",
       "|    └─ConvTranspose2d: 2-31             147,584\n",
       "├─Sequential: 1-14                       --\n",
       "|    └─Conv2d: 2-32                      1,153\n",
       "|    └─BatchNorm2d: 2-33                 2\n",
       "|    └─ReLU: 2-34                        --\n",
       "├─Sequential: 1-15                       --\n",
       "|    └─AvgPool2d: 2-35                   --\n",
       "├─Sequential: 1-16                       --\n",
       "|    └─Conv2d: 2-36                      640\n",
       "|    └─BatchNorm2d: 2-37                 128\n",
       "|    └─ReLU: 2-38                        --\n",
       "├─Sequential: 1-17                       --\n",
       "|    └─Conv2d: 2-39                      73,856\n",
       "|    └─BatchNorm2d: 2-40                 256\n",
       "|    └─ReLU: 2-41                        --\n",
       "├─Sequential: 1-18                       --\n",
       "|    └─Conv2d: 2-42                      147,584\n",
       "|    └─BatchNorm2d: 2-43                 256\n",
       "|    └─ReLU: 2-44                        --\n",
       "├─Sequential: 1-19                       --\n",
       "|    └─ConvTranspose2d: 2-45             1,153\n",
       "=================================================================\n",
       "Total params: 1,629,956\n",
       "Trainable params: 1,629,956\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SCNET(nn.Module): # need to add bottleneck\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        # Local appearance\n",
    "\n",
    "        # block 1\n",
    "        self.conv1 = self.conv_block1(in_channels, 64, 3, 1)\n",
    "        self.conv2 = self.conv_block1(64, 128, 3, 1)\n",
    "        self.conv3 = self.conv_block1(128,128,3, 1)\n",
    "\n",
    "        # block 2\n",
    "        self.pool_block_2 = self.pool_layer_block_2()\n",
    "\n",
    "        # block 3\n",
    "        self.conv4 = self.conv_block3(128, 128, 3, 1) # note conv4 acts on pool, which acts on conv2\n",
    "        self.conv5 = self.conv_block3(128, 128, 3, 1)\n",
    "        self.conv6 = self.conv_block3(128,128,3, 1)\n",
    "\n",
    "        # block 4\n",
    "        self.pool_block_4 = self.pool_layer_block_4()\n",
    "\n",
    "        # block 5\n",
    "        self.conv7 = self.conv_block5(128, 128, 3, 1) # note conv7 acts on pool, which acts on conv5\n",
    "        self.conv8 = self.conv_block5(128, 128, 3, 1)\n",
    "        self.conv9 = self.conv_block5(128,128,3, 1)\n",
    "\n",
    "        # block 6\n",
    "        self.upsample_block_6 = self.upsample_layer_block_6(128)\n",
    "\n",
    "        # block 7\n",
    "        # addition\n",
    "        self.upsample_block_7 = self.upsample_layer_block_7(128)\n",
    "\n",
    "\n",
    "        # block 8\n",
    "        # addition\n",
    "        self.conv10 = self.conv_block8(128, 1, 3, 1)\n",
    "\n",
    "        # Spatial configuration\n",
    "\n",
    "        # block 9\n",
    "        self.pool_block_9 = self.pool_layer_block_9()\n",
    "\n",
    "        # block 10\n",
    "        self.conv11 = self.conv_block10(1, 64, 3, 1) \n",
    "        self.conv12 = self.conv_block10(64, 128, 3, 1)\n",
    "        self.conv13 = self.conv_block10(128,128,3, 1)\n",
    "\n",
    "        # block 11\n",
    "        self.upsample_block_11 = self.upsample_layer_block_11(128, 1)\n",
    "\n",
    "        # block 12\n",
    "        # multiplication\n",
    "\n",
    "\n",
    "    def __call__(self, x):\n",
    "\n",
    "        # block 1\n",
    "        conv1 = self.conv1(x)\n",
    "        conv2 = self.conv2(conv1)\n",
    "        conv3 = self.conv3(conv2)\n",
    "\n",
    "        # block 2\n",
    "        pool_block_2 = self.pool_block_2(conv2) # note middle one\n",
    "\n",
    "        # block 3\n",
    "        conv4 = self.conv4(pool_block_2)\n",
    "        conv5 = self.conv5(conv4)\n",
    "        conv6 = self.conv6(conv5)\n",
    "\n",
    "        # block 4\n",
    "        pool_block_4 = self.pool_block_4(conv5)\n",
    "\n",
    "        # block 5\n",
    "        conv7 = self.conv7(pool_block_4)\n",
    "        conv8 = self.conv8(conv7)\n",
    "        conv9 = self.conv9(conv8)\n",
    "\n",
    "        # block 6\n",
    "        upsample_block_6 = self.upsample_block_6(conv9)\n",
    "\n",
    "        # block 7\n",
    "        add_block_7 = conv6 + upsample_block_6\n",
    "        upsample_block_7 = self.upsample_block_7(add_block_7)\n",
    "\n",
    "        # block 8\n",
    "        add_block_8 = upsample_block_7 + conv3\n",
    "        conv10 = self.conv10(add_block_8)\n",
    "\n",
    "        # block 9\n",
    "        pool_block_9 = self.pool_block_9(conv10)\n",
    "\n",
    "        # block 10\n",
    "        conv11 = self.conv11(pool_block_9)\n",
    "        conv12 = self.conv12(conv11)\n",
    "        conv13 = self.conv13(conv12)\n",
    "\n",
    "        # block 11\n",
    "        upsample_block_11 = self.upsample_block_11(conv13)\n",
    "\n",
    "        # block 12\n",
    "        output = upsample_block_11 * conv10\n",
    "\n",
    "        return output\n",
    "\n",
    "    def conv_block1(self, in_channels, out_channels, kernel_size, padding):\n",
    "\n",
    "        conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size = kernel_size, stride = 1, padding = padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        return conv_block1\n",
    "\n",
    "    def pool_layer_block_2(self):\n",
    "\n",
    "        pool_layer_block_2 = nn.Sequential(\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        )\n",
    "\n",
    "        return pool_layer_block_2\n",
    "    \n",
    "    def conv_block3(self, in_channels, out_channels, kernel_size, padding):\n",
    "\n",
    "        conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size = kernel_size, stride = 1, padding = padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        return conv_block2\n",
    "\n",
    "    def pool_layer_block_4(self):\n",
    "\n",
    "        pool_layer_block_4 = nn.Sequential(\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        )\n",
    "\n",
    "        return pool_layer_block_4\n",
    "    \n",
    "    def conv_block5(self, in_channels, out_channels, kernel_size, padding):\n",
    "\n",
    "        conv_block5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size = kernel_size, stride = 1, padding = padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        return conv_block5\n",
    "    \n",
    "    def upsample_layer_block_6(self, channels):\n",
    "\n",
    "        upsample_block_6 = nn.Sequential(\n",
    "          nn.ConvTranspose2d(channels, channels, kernel_size = 3, stride = 2, padding = 1, output_padding=1) #??\n",
    "        )\n",
    "        return upsample_block_6\n",
    "    \n",
    "    def upsample_layer_block_7(self, channels):\n",
    "\n",
    "        upsample_block_7 = nn.Sequential(\n",
    "          nn.ConvTranspose2d(channels, channels, kernel_size = 3, stride = 2, padding = 1, output_padding=1) #??\n",
    "        )\n",
    "        return upsample_block_7\n",
    "\n",
    "    def conv_block8(self, in_channels, out_channels, kernel_size, padding):\n",
    "\n",
    "        conv_block8 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size = kernel_size, stride = 1, padding = padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        return conv_block8\n",
    "\n",
    "    def pool_layer_block_9(self):\n",
    "\n",
    "        pool_layer_block_9 = nn.Sequential(\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        )\n",
    "\n",
    "        return pool_layer_block_9\n",
    "\n",
    "    def conv_block10(self, in_channels, out_channels, kernel_size, padding):\n",
    "\n",
    "        conv_block10 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size = kernel_size, stride = 1, padding = padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        return conv_block10\n",
    "\n",
    "    def upsample_layer_block_11(self, in_channels, out_channels):\n",
    "\n",
    "        upsample_block_11 = nn.Sequential(\n",
    "          nn.ConvTranspose2d(in_channels, out_channels, kernel_size = 3, stride = 2, padding = 1, output_padding=1) #??\n",
    "        )\n",
    "        return upsample_block_11\n",
    "\n",
    "\n",
    "# Model summary\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SCNET(1,1)\n",
    "model = model.to(device)\n",
    "\n",
    "from torchsummary import summary\n",
    "summary(model, input_size=(1, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and train functions\n",
    "\n",
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def point_to_point(mask_x, mask_y, pred_x, pred_y):\n",
    "  # calculates point to point error for image\n",
    "  point_to_point = ((pred_x - mask_x)**2 + (pred_y - mask_y)**2)**0.5\n",
    "  return point_to_point\n",
    "\n",
    "# returns gaussian function \n",
    "def gaussian(x,y, targ_coords, sigma, gamma, dimension = 2): # assumes in 2d space\n",
    "  # x, y are general coords and targ_coords define mean of gaussian\n",
    "  x_targ, y_targ = targ_coords[0], targ_coords[1]\n",
    "  l2_dif = torch.tensor(-((x-x_targ)**2 + (y- y_targ)**2)/(2*sigma**2))\n",
    "  gauss = torch.tensor((gamma) * (2*np.pi)**(-dimension/2) * sigma ** (-dimension) ** torch.exp(l2_dif))\n",
    "  return gauss\n",
    "\n",
    "def gaussian_map(peak_x,peak_y,sigma,gamma,x_size,y_size,output,dimension = 2): # 2D gaussian 5x5 image\n",
    "  if output == False: # expands gaussian map to 448x448 to ensure proper normalisation ? not working\n",
    "    y,x = np.ogrid[0:2*y_size,0:2*x_size]\n",
    "    peak_x = peak_x + x_size/2\n",
    "    peak_y = peak_y + y_size/2\n",
    "    pre_factor = ((gamma) * (2*np.pi)**(-dimension/2) * sigma ** (-dimension)) \n",
    "    h = pre_factor * torch.exp( -((torch.tensor(x).to(device)-peak_x)**2 + (torch.tensor(y).to(device)-peak_y)**2) / (2.*sigma*sigma) )\n",
    "  if output == True:\n",
    "    y,x = np.ogrid[0:y_size,0:x_size]\n",
    "    pre_factor = ((gamma) * (2*np.pi)**(-dimension/2) * sigma ** (-dimension)) \n",
    "    h = pre_factor * torch.exp( -((torch.tensor(x).to(device)-peak_x)**2 + (torch.tensor(y).to(device)-peak_y)**2) / (2.*sigma*sigma) )\n",
    "  return h\n",
    "\n",
    "# L2 loss for gaussian heat maps\n",
    "def calc_loss_gauss(pred, target, target_centre, metrics, alpha, reg, gamma, epoch_samples, sigma): \n",
    "    # loss function for difference between gaussian heat maps\n",
    "    # pred is gaussian heatmap & need to convert target to gaussian heatmap\n",
    "    target_batch_coords = coords_max(target_centre)\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    batch_size = target_batch_coords.size()[0]\n",
    "    total_batch_loss = 0\n",
    "    total_sum_loss = 0\n",
    "    total_reg_loss = 0\n",
    "    total_alpha_loss = 0\n",
    "    total_x_posn = 0\n",
    "    total_y_posn = 0\n",
    "    total_x_targ_posn = 0\n",
    "    total_y_targ_posn = 0\n",
    "    total_point_to_point = 0\n",
    "    \n",
    "    \n",
    "    for i in range(batch_size): # i.e. for each image/mask in batch\n",
    "      targ_coords = target_batch_coords[i]\n",
    "      img_number = epoch_samples + i # epoch_samples is 0, 32, 64 e.g. if batch is size 32\n",
    "\n",
    "      # because of normalisation need to make sure not putting mask at 0,0\n",
    "      if (targ_coords[0] == 0) or (targ_coords[1] == 0) or (targ_coords[0] == 223) or (targ_coords[1] == 223):\n",
    "        print('----------------------ERROR-----------------------')\n",
    "      \n",
    "      x_size = target.size()[3]\n",
    "      y_size = target.size()[2] \n",
    "\n",
    "      # average x/y posn\n",
    "      total_x_posn += coords_max(pred)[i][0] # change\n",
    "      total_y_posn += coords_max(pred)[i][1] # change\n",
    "      total_x_targ_posn += targ_coords[0]\n",
    "      total_y_targ_posn += targ_coords[1]\n",
    "\n",
    "      # average point to point\n",
    "      total_point_to_point += point_to_point(targ_coords[0],targ_coords[1],coords_max(pred)[i][0],coords_max(pred)[i][1]) # change\n",
    "     \n",
    "      # note how this is True because need targ_gaus to be 224x224 -> need to convert to potentially making a 448x448 image from output\n",
    "      targ_gaus = gaussian_map(targ_coords[0],targ_coords[1],sigma,gamma,x_size,y_size,output = True) \n",
    "\n",
    "      # try normalising mean and targ gaus see if helps\n",
    "      pred_heatmap = pred[i].squeeze()\n",
    "      #pred_heatmap_norm = (pred_heatmap - torch.mean(pred_heatmap))/torch.std(pred_heatmap)\n",
    "      #targ_gaus = (targ_gaus - torch.mean(targ_gaus))/torch.std(targ_gaus)\n",
    "\n",
    "      img_loss = ((((pred_heatmap - targ_gaus)**2)).sum()) # multiply by targ gaus for box normalisation\n",
    "      sum_loss = ((((pred_heatmap - targ_gaus)**2)).sum())\n",
    "      \n",
    "      # regularization term\n",
    "      squ_weights = torch.tensor(0,dtype=torch.float64).to(device)\n",
    "      for model_param_name, model_param_value in model.named_parameters():\n",
    "        if model_param_name.endswith('weight'):\n",
    "          squ_weights += (model_param_value.norm())**2\n",
    "      \n",
    "      regularization = (reg * squ_weights)\n",
    "      reg_loss = regularization\n",
    "      \n",
    "      img_loss += alpha * (sigma.norm())**2 + regularization\n",
    "      alpha_loss = alpha * (sigma.norm())**2\n",
    "\n",
    "      # add to loss\n",
    "      total_batch_loss += img_loss\n",
    "      total_sum_loss += sum_loss\n",
    "      total_reg_loss += reg_loss\n",
    "      total_alpha_loss += alpha_loss\n",
    "\n",
    "      # print for every epoch_samples = 0 -> i.e first image in epoch\n",
    "      if epoch_samples == 0:\n",
    "        print(' ---- first image of set ---- (start)')\n",
    "        print('targ: (%5.2f,%5.2f)' % (targ_coords[0],targ_coords[1]))\n",
    "        print('pred: (%5.2f,%5.2f)' % (coords_max(pred)[i][0],coords_max(pred)[i][1])) # change\n",
    "        print('point to point')\n",
    "        print(point_to_point(targ_coords[0],targ_coords[1],coords_max(pred)[i][0],coords_max(pred)[i][1])) # change\n",
    "        print('img loss')\n",
    "        print(img_loss)\n",
    "        pred_heatmap = pred_heatmap.unsqueeze(0)\n",
    "        targ_gaus = targ_gaus.unsqueeze(0)\n",
    "        print('predicted heatmap')\n",
    "        plt.figure()\n",
    "        plt.imshow(reverse_transform(pred_heatmap.detach().cpu()),cmap = 'viridis',alpha = 1)\n",
    "        plt.show()\n",
    "        print('target heatmap')\n",
    "        plt.figure()\n",
    "        plt.imshow(reverse_transform(targ_gaus.detach().cpu()),cmap = 'viridis',alpha = 1)\n",
    "        plt.show()\n",
    "\n",
    "          \n",
    "    # return mean batch loss\n",
    "    mean_batch_loss = (total_batch_loss/batch_size)\n",
    "    mean_sum_loss = (total_sum_loss/batch_size)\n",
    "    mean_reg_loss = (total_reg_loss/batch_size)\n",
    "    mean_alpha_loss = (total_alpha_loss/batch_size)\n",
    "\n",
    "    # calculate moving average of position of prediction\n",
    "    mean_x_pred_posn = total_x_posn/batch_size\n",
    "    mean_y_pred_posn = total_y_posn/batch_size\n",
    "    mean_x_targ_posn = total_x_targ_posn/batch_size\n",
    "    mean_y_targ_posn = total_y_targ_posn/batch_size\n",
    "\n",
    "    # calculate average point to point\n",
    "    mean_point_to_point = total_point_to_point/batch_size\n",
    "\n",
    "\n",
    "    metrics['loss'] += mean_batch_loss.data.cpu().numpy() * target.size(0) # ?\n",
    "    metrics['sum loss'] += mean_sum_loss.data.cpu().numpy() * target.size(0)\n",
    "    metrics['reg loss'] += mean_reg_loss.data.cpu().numpy() * target.size(0)\n",
    "    metrics['alpha loss'] += mean_alpha_loss.data.cpu().numpy() * target.size(0)\n",
    "    metrics['mean x pred'] += mean_x_pred_posn.data.cpu().numpy() * target.size(0)\n",
    "    metrics['mean y pred'] += mean_y_pred_posn.data.cpu().numpy() * target.size(0)\n",
    "    metrics['mean x targ'] += mean_x_targ_posn.data.cpu().numpy() * target.size(0)\n",
    "    metrics['mean y targ'] += mean_y_targ_posn.data.cpu().numpy() * target.size(0)\n",
    "    metrics['mean point to point'] += mean_point_to_point * target.size(0)\n",
    "    \n",
    "    return mean_batch_loss\n",
    "\n",
    "def print_metrics(metrics, epoch_samples, phase):\n",
    "    outputs = []\n",
    "    for k in metrics.keys():\n",
    "        outputs.append(\"{}: {:4f}\".format(k, metrics[k] / epoch_samples))\n",
    "\n",
    "    print(\"{}: {}\".format(phase, \", \".join(outputs)))\n",
    "\n",
    "def train_model(model, optimizer, scheduler,alpha,reg,gamma,sigma,num_epochs,):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 1e10\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        since = time.time()\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    print(\"LR\", param_group['lr'])\n",
    "\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            metrics = defaultdict(float)\n",
    "            \n",
    "            epoch_samples = 0\n",
    "\n",
    "            for batch in dataloaders[phase]:\n",
    "                inputs = batch['image']\n",
    "                labels = batch['mask']\n",
    "                labels_centre = batch['mask_centre']\n",
    "               # print(labels.size())\n",
    "                inputs = inputs.float().to(device)\n",
    "                labels = labels.float().to(device)\n",
    "                labels_centre = labels_centre.float().to(device)\n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                #print(optimizer.parameter())\n",
    "                #sigma.zero_grad\n",
    "\n",
    "                # forward\n",
    "                # track history only if in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    \n",
    "                    outputs = model((inputs))\n",
    "\n",
    "\n",
    "                    # 1. convert masks to heatmaps inside loss function (allows sigma optimisation)\n",
    "                    loss = calc_loss_gauss(outputs, labels,labels_centre, metrics,alpha,reg,gamma,epoch_samples,sigma)\n",
    "                    # print image for comparison\n",
    "                    if epoch_samples == 0:\n",
    "                      #inputs = inputs.unsqueeze(0)\n",
    "                      inputs = inputs.squeeze(0)\n",
    "                      print(inputs.shape)\n",
    "                      print('image')\n",
    "                      plt.figure()\n",
    "                      plt.imshow(reverse_transform(inputs.detach().cpu()),cmap = 'Greys_r',alpha = 1)\n",
    "                      plt.show()\n",
    "                      print(' ---- first image of set ---- (end)')\n",
    "                    # 2. vs convert to heatmap here means no sigma optimisation\n",
    "                    \n",
    "                \n",
    "                    #print('maxcoords mask' + str(coords_max(outputs)))\n",
    "                    #print('maxcords label' + str(coords_max(labels)))\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        \n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        scheduler.step()\n",
    "\n",
    "                # statistics\n",
    "                epoch_samples += inputs.size(0)\n",
    "                \n",
    "            print_metrics(metrics, epoch_samples, phase)\n",
    "            #print('The following have zero requires grad:')\n",
    "            #all_have_grad = True\n",
    "            #for name, param in model.named_parameters():\n",
    "            #  if param.requires_grad == False:\n",
    "            #      print (name)\n",
    "            #      all_have_grad = False\n",
    "            #if (all_have_grad == True):\n",
    "            #  print('All parameters have require grad = true')      \n",
    "            print('Sigma is')\n",
    "            print(sigma)\n",
    "            \n",
    "            \n",
    "            epoch_loss = metrics['loss'] / epoch_samples\n",
    "            \n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                print(\"saving best model\")\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                \n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    print('Best val loss: {:4f}'.format(best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "def print_img_no_heatmaps(image, i):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    numpy_img = reverse_transform(image.cpu()) \n",
    "    # print image,mask,pred overlay\n",
    "    fig.add_subplot(1,batch_size+1, i+1) # 111 is lowest number\n",
    "    plt.imshow(numpy_img, cmap='Greys_r',vmin=numpy_img.min(), vmax=numpy_img.max(),alpha = 1.0)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def print_heatmaps(image,mask,mask_centre,pred,i):\n",
    "      fig = plt.figure(figsize=(10,10))\n",
    "      numpy_img = reverse_transform(image.cpu()) \n",
    "      numpy_mask = reverse_transform(mask.cpu())\n",
    "      numpy_pred = np.squeeze(pred.cpu().detach())\n",
    "      # print image,mask,pred overlay\n",
    "      fig.add_subplot(1,batch_size+1, i+1) # 111 is lowest number\n",
    "      plt.imshow(numpy_img, cmap='Greys_r',vmin=numpy_img.min(), vmax=numpy_img.max(),alpha = 1.0)\n",
    "      plt.imshow(numpy_mask,cmap='Greys_r', alpha = 0.5)\n",
    "      plt.imshow(numpy_pred,cmap=cm.jet, alpha = 0.5) \n",
    "      mask_max_coords = coords_max(mask_centre.unsqueeze(0)) # as need it to have 1 x 1 x H x W\n",
    "      # use fitted gauss\n",
    "      pred_max_coords = gauss_max(pred.unsqueeze(0)) # AMENDMENT\n",
    "      mask_max_x, mask_max_y = mask_max_coords[0][0],mask_max_coords[0][1] \n",
    "      pred_max_x, pred_max_y = pred_max_coords[0][0],pred_max_coords[0][1] \n",
    "      plt.plot(mask_max_x,mask_max_y,'rx',label = 'mask')\n",
    "      plt.plot(pred_max_x,pred_max_y,'bx',label = 'pred')\n",
    "      plt.legend()\n",
    "      plt.show()\n",
    "\n",
    "def print_gaus_heatmap(image,mask_centre,pred,i,batch_number,batch_size,sigma):\n",
    "  # needs to print gaussian heatmap of prediction over the image\n",
    "  fig = plt.figure(figsize=(10,10))\n",
    "  fig.add_subplot(1,batch_size+1, i+1) # smallest number is 111\n",
    "  # image\n",
    "  numpy_img = reverse_transform(image.cpu()) \n",
    "  plt.imshow(numpy_img, cmap='Greys_r',vmin=numpy_img.min(), vmax=numpy_img.max(),alpha = 1.0)\n",
    "  # mask\n",
    "  mask_max_x, mask_max_y = coords_max(mask_centre.unsqueeze(0))[0][0], coords_max(mask_centre.unsqueeze(0))[0][1]\n",
    "  plt.plot(mask_max_x,mask_max_y,'rx',label = 'mask')\n",
    "  #print('mask x and y are' + str(mask_max_x) + str(mask_max_y))\n",
    "  #plt.imshow(reverse_transform(mask.cpu()),cmap='Greys_r', alpha = 0.5)\n",
    "  # prediction\n",
    "  # use fitted gauss\n",
    "  pred_max_x, pred_max_y = gauss_max(pred.unsqueeze(0))[0][0], gauss_max(pred.unsqueeze(0))[0][1] # AMENDMENT\n",
    "  img_number = batch_number * batch_size + i\n",
    "  x_size = image.size()[2]\n",
    "  y_size = image.size()[1] \n",
    "\n",
    "  pred_gaus_heatmap = gaussian_map(pred_max_x,pred_max_y,sigma,gamma,x_size,y_size,output = True,dimension = 2)\n",
    "  numpy_pred = np.squeeze(pred_gaus_heatmap.detach().cpu())\n",
    "  plt.imshow(numpy_pred,cmap=cm.jet, alpha = 0.5) \n",
    "  plt.plot(pred_max_x,pred_max_y,'bx', label = 'pred')\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def performance_metrics(model,sigma,gamma):\n",
    "  #fig = plt.figure()\n",
    "  point_to_point_array = []\n",
    "  outliers = 0\n",
    "  for batch in dataloaders['test']:\n",
    "    image = batch['image'].to(device)\n",
    "    mask = batch['mask'].to(device)\n",
    "    mask_centre = batch['mask_centre'].to(device)\n",
    "    pred = model(image)\n",
    "\n",
    "    # apply sigmoid to mask and pred ?\n",
    "    #mask = F.sigmoid(mask)\n",
    "    #pred = F.sigmoid(pred)\n",
    "    \n",
    "    batch_number = 0\n",
    "\n",
    "    for i in range(batch_size):\n",
    "      mask_coords_max = coords_max(mask_centre)\n",
    "      # change to gauss fit!!!\n",
    "      #pred_coords_max = gauss_max(pred) # change\n",
    "      pred_coords_max = coords_max(pred)\n",
    "      #print('max of prediction is' + str(pred_coords_max))\n",
    "      mask_max_x, mask_max_y = mask_coords_max[i][0],mask_coords_max[i][1] \n",
    "      pred_max_x, pred_max_y =  pred_coords_max[i][0], pred_coords_max[i][1] # change\n",
    "      print_heatmaps(image[i], mask[i],mask_centre[i], pred[i], i)\n",
    "      print_gaus_heatmap(image[i],mask_centre[i],pred[i],i,batch_number,batch_size,sigma)\n",
    "      print_img_no_heatmaps(image[i], i)# print image on its own\n",
    "      img_point_to_point = point_to_point(mask_max_x, mask_max_y, pred_max_x, pred_max_y)\n",
    "      point_to_point_array.append(img_point_to_point)\n",
    "      # if img_point_to_point > 20mm is an outlier\n",
    "      if img_point_to_point > 20:\n",
    "        outliers += 1\n",
    "\n",
    "    batch_number += 1\n",
    "\n",
    "  mean = np.mean(point_to_point_array)\n",
    "  std_mean = np.std(point_to_point_array,ddof =1)*(len(point_to_point_array))**-0.5\n",
    "  median = np.median(point_to_point_array)\n",
    "  outliers_perc = outliers/len(point_to_point_array) * 100\n",
    "  print('mean point to point error is ' + str(mean) + '+/-' + str(std_mean))\n",
    "  print('median point to point error is ' + str(median))\n",
    "  print('percentage of images which were outliers is ' + str(outliers_perc) + '%')\n",
    "  print('sigma is ' + str(sigma))\n",
    "  plt.show()\n",
    "\n",
    "# weight initialisation\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu') # change if switch to ReLU\n",
    "        torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "H6AMNvhgyurM"
   },
   "outputs": [],
   "source": [
    "# L2 loss for gaussian heat maps\n",
    "def calc_loss_MSE(pred, target,target_centre, metrics, alpha, reg, gamma, epoch_samples, sigma): \n",
    "    # loss function for difference between gaussian heat maps\n",
    "    # pred is gaussian heatmap & need to convert target to gaussian heatmap\n",
    "    target_batch_coords = coords_max(target_centre)\n",
    "   \n",
    "    batch_size = target_batch_coords.size()[0]\n",
    "    total_batch_loss = 0\n",
    "    total_sum_loss = 0\n",
    "    total_reg_loss = 0\n",
    "    total_alpha_loss = 0\n",
    "    total_x_posn = 0\n",
    "    total_y_posn = 0\n",
    "    total_x_targ_posn = 0\n",
    "    total_y_targ_posn = 0\n",
    "    total_point_to_point = 0\n",
    "    \n",
    "    \n",
    "    for i in range(batch_size): # i.e. for each image/mask in batch\n",
    "      targ_coords = target_batch_coords[i]\n",
    "      img_number = epoch_samples + i # epoch_samples is 0, 32, 64 e.g. if batch is size 32\n",
    "      #sigmas['spine'] = torch.tensor([20.], requires_grad=True).to(device) # what value to initialise sigma?\n",
    "      \n",
    "      x_size = target.size()[3]\n",
    "      y_size = target.size()[2] \n",
    "\n",
    "      # average x/y posn\n",
    "      total_x_posn += coords_max(pred)[i][0] # change\n",
    "      total_y_posn += coords_max(pred)[i][1] # change\n",
    "      total_x_targ_posn += targ_coords[0]\n",
    "      total_y_targ_posn += targ_coords[1]\n",
    "\n",
    "      # average point to point\n",
    "      total_point_to_point += point_to_point(targ_coords[0],targ_coords[1],coords_max(pred)[i][0],coords_max(pred)[i][1]) # change\n",
    "\n",
    "     \n",
    "      # note how this is True because need targ_gaus to be 224x224 -> need to convert to potentially making a 448x448 image from output\n",
    "      targ_gaus = gaussian_map(targ_coords[0],targ_coords[1],sigma,gamma,x_size,y_size,output = True) \n",
    "\n",
    "      # normalise\n",
    "      targ_gaus = (targ_gaus - torch.mean(targ_gaus))/torch.std(targ_gaus)\n",
    "      pred_heatmap = pred[i].squeeze()\n",
    "      pred_norm_heatmap = (pred_heatmap - torch.mean(pred_heatmap))/torch.std(pred_heatmap)\n",
    "\n",
    "      img_loss_fn = nn.MSELoss()\n",
    "      img_loss = img_loss_fn(pred_norm_heatmap,targ_gaus)\n",
    "      sum_loss = img_loss_fn(pred_norm_heatmap,targ_gaus)\n",
    "      \n",
    "      # add to loss\n",
    "      total_batch_loss += img_loss\n",
    "      total_sum_loss += sum_loss\n",
    "\n",
    "          \n",
    "    # return mean batch loss\n",
    "    mean_batch_loss = (total_batch_loss/batch_size)\n",
    "    mean_sum_loss = (total_sum_loss/batch_size)\n",
    "\n",
    "    # calculate moving average of position of prediction\n",
    "    mean_x_pred_posn = total_x_posn/batch_size\n",
    "    mean_y_pred_posn = total_y_posn/batch_size\n",
    "    mean_x_targ_posn = total_x_targ_posn/batch_size\n",
    "    mean_y_targ_posn = total_y_targ_posn/batch_size\n",
    "\n",
    "    # calculate average point to point\n",
    "    mean_point_to_point = total_point_to_point/batch_size\n",
    "\n",
    "\n",
    "    metrics['loss'] += mean_batch_loss.data.cpu().numpy() * target.size(0) # ?\n",
    "    metrics['sum loss'] += mean_sum_loss.data.cpu().numpy() * target.size(0)\n",
    "    metrics['mean x pred'] += mean_x_pred_posn.data.cpu().numpy() * target.size(0)\n",
    "    metrics['mean y pred'] += mean_y_pred_posn.data.cpu().numpy() * target.size(0)\n",
    "    metrics['mean x targ'] += mean_x_targ_posn.data.cpu().numpy() * target.size(0)\n",
    "    metrics['mean y targ'] += mean_y_targ_posn.data.cpu().numpy() * target.size(0)\n",
    "    metrics['mean point to point'] += mean_point_to_point * target.size(0)\n",
    "    \n",
    "    return mean_batch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "vCZIVDw9_b8T"
   },
   "outputs": [],
   "source": [
    "def calc_loss(pred, target,target_centre, metrics,alpha, reg, gamma, epoch_samples,sigma): \n",
    "    # loss function for difference between gaussian heat maps\n",
    "    # pred and target are heatmaps\n",
    "    target_batch_coords = coords_max(target_centre)\n",
    "    pred_batch_coords = coords_max(pred) # change\n",
    "    \n",
    "    batch_size = target_batch_coords.size()[0]\n",
    "    total_batch_loss = 0\n",
    "    total_sum_loss = 0\n",
    "    total_reg_loss = 0\n",
    "    total_alpha_loss = 0\n",
    "    for i in range(batch_size): # i.e. for each image/mask in batch\n",
    "      targ_coords = target_batch_coords[i]\n",
    "      pred_coords_max = pred_batch_coords[i] # change\n",
    "\n",
    "      img_loss = torch.tensor((pred_coords_max[0]-targ_coords[0])**2 + (pred_coords_max[1]-targ_coords[1])**2,dtype = torch.float64).to(device) # change\n",
    "      sum_loss = (pred_coords_max[0]-targ_coords[0])**2 + (pred_coords_max[1]-targ_coords[1])**2 # change\n",
    "\n",
    "      # regularization term\n",
    "      squ_weights = torch.tensor(0,dtype=torch.float64).to(device)\n",
    "      for model_param_name, model_param_value in model.named_parameters():\n",
    "        if model_param_name.endswith('weight'):\n",
    "          squ_weights += (model_param_value.norm())**2\n",
    "      \n",
    "      regularization = (reg * squ_weights)\n",
    "      reg_loss = regularization\n",
    "      \n",
    "      img_loss += alpha * (sigma.norm())**2 + regularization\n",
    "      alpha_loss = alpha * (sigma.norm())**2\n",
    "      \n",
    "      \n",
    "      # add to loss\n",
    "      total_batch_loss += img_loss\n",
    "      total_sum_loss += sum_loss\n",
    "      total_reg_loss += reg_loss\n",
    "      total_alpha_loss += alpha_loss\n",
    "      \n",
    "    \n",
    "    # return mean batch loss\n",
    "    mean_batch_loss = (total_batch_loss/batch_size)\n",
    "    mean_sum_loss = (total_sum_loss/batch_size)\n",
    "    mean_reg_loss = (total_reg_loss/batch_size)\n",
    "    mean_alpha_loss = (total_alpha_loss/batch_size)\n",
    "\n",
    "    #metrics['bce'] += bce.data.cpu().numpy() * target.size(0)\n",
    "    #metrics['dice'] += dice.data.cpu().numpy() * target.size(0)\n",
    "    metrics['loss'] += mean_batch_loss.data.cpu().numpy() * target.size(0) # ?\n",
    "    metrics['sum loss'] += mean_sum_loss.data.cpu().numpy() * target.size(0)\n",
    "    metrics['reg loss'] += mean_reg_loss.data.cpu().numpy() * target.size(0)\n",
    "    metrics['alpha loss'] += mean_alpha_loss.data.cpu().numpy() * target.size(0)\n",
    "    \n",
    "    \n",
    "    return mean_batch_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 130
    },
    "id": "uB0mo1blmMNd",
    "outputId": "0b72d5c0-77da-4df1-e4ed-d86d6033f70c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# check masks being loaded in correctly\\nfor batch in dataloaders['test']:\\n  mask = batch['mask']\\n  image = batch['image']\\n  mask_centre = batch['mask_centre']\\n  mask_max_coords = coords_max(mask)\\n  mask_centre_max_coords = coords_max(mask_centre)\\n  #print(mask_max_coords)\\n  #print(mask_centre_max_coords)\\n\\n  mask_max_x, mask_max_y = mask_centre_max_coords[0][0],mask_centre_max_coords[0][1]\\n  if (mask_max_x < 100) or (mask_max_y < 73) or (mask_max_x > 150) or (mask_max_y > 150):\\n    torch.set_printoptions(precision=20)\\n    print(torch.max(mask[0][0]))\\n    print(torch.min(mask[0][0]))\\n    numpy_img = reverse_transform(image[0]) \\n    numpy_mask = reverse_transform(mask[0])\\n      #if (numpy_mask.min() == numpy_mask.max()):\\n    print('x:%5.2f,y:%5.2f' % (mask_max_x,mask_max_y))\\n    fig= plt.figure()\\n    print(numpy_mask.min())\\n    print(numpy_mask.max())\\n    # print image,mask\\n    plt.imshow(numpy_img, cmap='Greys_r',vmin=numpy_img.min(), vmax=numpy_img.max(),alpha = 1.0)\\n    plt.imshow(numpy_mask,cmap='Greys_r', alpha = 1.0)\\n    plt.plot(mask_max_x,mask_max_y,'ro',fillstyle='none', markersize = 15,label = 'mask')\\n    plt.show()\\n  \""
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# check masks being loaded in correctly\n",
    "for batch in dataloaders['test']:\n",
    "  mask = batch['mask']\n",
    "  image = batch['image']\n",
    "  mask_centre = batch['mask_centre']\n",
    "  mask_max_coords = coords_max(mask)\n",
    "  mask_centre_max_coords = coords_max(mask_centre)\n",
    "  #print(mask_max_coords)\n",
    "  #print(mask_centre_max_coords)\n",
    "\n",
    "  mask_max_x, mask_max_y = mask_centre_max_coords[0][0],mask_centre_max_coords[0][1]\n",
    "  if (mask_max_x < 100) or (mask_max_y < 73) or (mask_max_x > 150) or (mask_max_y > 150):\n",
    "    torch.set_printoptions(precision=20)\n",
    "    print(torch.max(mask[0][0]))\n",
    "    print(torch.min(mask[0][0]))\n",
    "    numpy_img = reverse_transform(image[0]) \n",
    "    numpy_mask = reverse_transform(mask[0])\n",
    "      #if (numpy_mask.min() == numpy_mask.max()):\n",
    "    print('x:%5.2f,y:%5.2f' % (mask_max_x,mask_max_y))\n",
    "    fig= plt.figure()\n",
    "    print(numpy_mask.min())\n",
    "    print(numpy_mask.max())\n",
    "    # print image,mask\n",
    "    plt.imshow(numpy_img, cmap='Greys_r',vmin=numpy_img.min(), vmax=numpy_img.max(),alpha = 1.0)\n",
    "    plt.imshow(numpy_mask,cmap='Greys_r', alpha = 1.0)\n",
    "    plt.plot(mask_max_x,mask_max_y,'ro',fillstyle='none', markersize = 15,label = 'mask')\n",
    "    plt.show()\n",
    "  \"\"\"\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JL5A10YuLWKd",
    "outputId": "6451c3d5-1245-4e0c-f7b5-a38c7c38f10a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# train model parameters\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import copy\n",
    "import math\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_class = 1 \n",
    "sigma = torch.tensor([30.]).to(device) # what value to initialise sigma?\n",
    "sigma.requires_grad =True\n",
    "print(sigma.is_leaf)\n",
    "\n",
    "alpha = 1/25000\n",
    "reg = 0.001\n",
    "gamma = 1000\n",
    "# sigmas = defaultdict(float)\n",
    "lr_max = 0.1\n",
    "lr_min = 0.0001\n",
    "step_size = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "139j8iPJRk6p"
   },
   "outputs": [],
   "source": [
    "# Paths load and save\n",
    "\n",
    "PATH_load = r'C:\\Users\\olive\\OneDrive\\Documents\\CNN\\Report\\model_scnet_50_norm.pt'\n",
    "PATH_opt_load = r'C:\\Users\\olive\\OneDrive\\Documents\\CNN\\Report\\model_scnet_opt_50_norm.pt'\n",
    "PATH_sigma_load = r'C:\\Users\\olive\\OneDrive\\Documents\\CNN\\Report\\model_scnet_sigma_50_norm.pt'\n",
    "#PATH_scaler_load = r'C:\\Users\\olive\\OneDrive\\Documents\\CNN\\Brainstem\\Crop\\model_scnet_scaler_24features.pt'\n",
    "#PATH_val_loss_load = r'C:\\Users\\olive\\OneDrive\\Documents\\CNN\\Brainstem\\Crop\\model_scnet_val_loss_24features.pt'\n",
    "\n",
    "\n",
    "PATH_save = r'C:\\Users\\olive\\OneDrive\\Documents\\CNN\\Report\\model_scnet_50_norm.pt'\n",
    "PATH_opt_save = r'C:\\Users\\olive\\OneDrive\\Documents\\CNN\\Report\\model_scnet_opt_50_norm.pt'\n",
    "PATH_sigma_save = r'C:\\Users\\olive\\OneDrive\\Documents\\CNN\\Report\\model_scnet_sigma_50_norm.pt'\n",
    "#PATH_scaler_save = r'C:\\Users\\olive\\OneDrive\\Documents\\CNN\\Brainstem\\Crop\\model_scnet_scaler_24features.pt'\n",
    "#PATH_val_loss_save = r'C:\\Users\\olive\\OneDrive\\Documents\\CNN\\Brainstem\\Crop\\model_scnet_val_loss_24features.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise model\n",
    "\n",
    "model = SCNET(1,num_class)\n",
    "model = model.to(device)\n",
    "# initialise weights\n",
    "\n",
    "sigma = torch.tensor([30.]).to(device) # what value to initialise sigma?\n",
    "sigma.requires_grad = True\n",
    "\n",
    "# initialise optimizer/scheduler/scaler\n",
    "optimizer = optim.Adam([\n",
    "                {'params': model.parameters()},\n",
    "                {'params': sigma},\n",
    "                #{'params': sigmas[3]} # not general\n",
    "            ], lr=1e-3, weight_decay = 0.05) # use adam lr optimiser\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=20000, gamma=0.1)\n",
    "#scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in a model\n",
    "\n",
    "# load in sigmas - not specific\n",
    "sigma = torch.tensor([30.]).to(device) # what value to initialise sigma?\n",
    "sigma.requires_grad = True\n",
    "sigma = torch.load(PATH_sigma_load)['sigma'] # what value to initialise sigma\n",
    "\n",
    "\n",
    "# load in model/optimizer/scaler\n",
    "model_load = SCNET(1,num_class)\n",
    "model_load = model_load.to(device)\n",
    "optimizer_load = optim.Adam([\n",
    "                {'params': model.parameters()},\n",
    "                {'params': sigma} # not general\n",
    "            ], lr=1e-3, weight_decay = 0.05) # use adam lr optimiser\n",
    "#scaler_load = torch.cuda.amp.GradScaler()\n",
    "\n",
    "model_load.load_state_dict(torch.load(PATH_load))\n",
    "optimizer_load.load_state_dict(torch.load(PATH_opt_load))\n",
    "#scaler_load.load_state_dict(torch.load(PATH_scaler_load))\n",
    "scheduler = lr_scheduler.StepLR(optimizer_load, step_size=20000, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train initialised model\n",
    "\n",
    "#best_loss = 1e10\n",
    "epoch_num = 50\n",
    "train_set.dataset.__train__() \n",
    "model = train_model(model, optimizer, scheduler, alpha,reg,gamma,sigma, num_epochs=epoch_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate initialised model\n",
    "\n",
    "!nvidia-smi\n",
    "model.eval()   # Set model to the evaluation mode\n",
    "test_set.dataset.__test__() # sets whole dataset to test mode means it doesn't augment images\n",
    "performance_metrics(model,sigma,gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train loaded in model\n",
    "\n",
    "# load in val loss\n",
    "best_loss = torch.load(PATH_val_loss_load)['best_val_loss']\n",
    "\n",
    "train_set.dataset.__train__() \n",
    "model_load = train_model(model_load, optimizer_load, scheduler, alpha,reg,gamma,sigma, num_epochs=epoch_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate loaded in model\n",
    "\n",
    "!nvidia-smi\n",
    "\n",
    "model_load.eval()\n",
    "test_set.dataset.__test__() # sets whole dataset to test mode means it doesn't augment images\n",
    "performance_metrics(model_load,sigma,gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "3GJXWHBnk_9j"
   },
   "outputs": [],
   "source": [
    "# save model to documents\n",
    "\n",
    "\n",
    "# careful if uncommenting below could overwrite!!!\n",
    "\"\"\"\n",
    "torch.save(model.state_dict(), PATH_save)\n",
    "torch.save(optimizer.state_dict(), PATH_opt_save)\n",
    "torch.save({\n",
    "            'sigma': sigma,\n",
    "            }, PATH_sigma_save) # change accordingly\n",
    "#torch.save(scaler.state_dict(), PATH_scaler_save)\n",
    "#torch.save({'best_val_loss': best_loss}, PATH_val_loss_save)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "2Dslices_unet_layers.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
